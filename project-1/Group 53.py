# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gN3Z9B62u3uIM9KdYWJr_OcnMq4_5ZoP
"""

import pandas as pd
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
import numpy as np

# Step 1: Load the saved model
dataset3_model = load_model('dataset3_model.h5')
dataset1_model = load_model('dataset1_model.h5')

# Step 2: Load your new dataset from a CSV file
new_data = pd.read_csv('valid_text_seq.csv')

# Step 3: Preprocess the new dataset
new_texts = new_data['input_str'].astype(str).tolist()  # Assuming the text column is named 'input_str'

# Step 4: Create and fit the tokenizer on the training data
# This should ideally match how you fitted it during training
# You can create a tokenizer with the same configuration if necessary
tokenizer = Tokenizer(char_level=True)  # Use char_level or word-level based on your original model setup
tokenizer.fit_on_texts(new_texts)  # Fit on the new dataset

# Step 5: Convert text data to sequences of integers
new_sequences = tokenizer.texts_to_sequences(new_texts)

# Step 6: Pad sequences to the same length used during training
max_length = 50  # This should match the maxlen used during training
new_padded = pad_sequences(new_sequences, maxlen=max_length, padding='post')

# Step 7: Make predictions using the loaded model
predictions = dataset3_model.predict(new_padded)

# Step 8: Convert predictions from probabilities to class labels
predicted_classes = (predictions > 0.5).astype(int)  # For binary classification

# Step 9: (Optional) Add predictions to the new data
new_data['predicted_label'] = predicted_classes

# Step 10: Save predictions to a new CSV file (optional)
new_data.to_csv('pred_text.txt', index=False)

print("Predictions made and saved to 'pred_text.csv'")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dropout, Dense, Input, Flatten
from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, f1_score, precision_score
from sklearn.ensemble import RandomForestClassifier
import time

# Read emoticon dataset
train_emoticon_df = pd.read_csv("train_emoticon.csv")
valid_emoticon_df = pd.read_csv("valid_emoticon.csv")
test_emoticon_df = pd.read_csv("test_emoticon.csv")

train_emoticon_X = train_emoticon_df['input_emoticon'].tolist()
train_emoticon_Y = train_emoticon_df['label'].tolist()

valid_emoticon_X = valid_emoticon_df['input_emoticon'].tolist()
valid_emoticon_Y = valid_emoticon_df['label'].tolist()

test_emoticon_X = test_emoticon_df['input_emoticon'].tolist()

# Step 1: Tokenization
tokenizer = Tokenizer(char_level=True, filters='')  # Treat each emoji as a character
tokenizer.fit_on_texts(train_emoticon_X)

# Convert emoticons to sequences of integers
sequences_train = tokenizer.texts_to_sequences(train_emoticon_X)
sequences_valid = tokenizer.texts_to_sequences(valid_emoticon_X)
sequences_test = tokenizer.texts_to_sequences(test_emoticon_X)

# Padding sequences to the same length
max_len = max(len(seq) for seq in sequences_train)  # Get max length from training data
padded_sequences_train = pad_sequences(sequences_train, maxlen=max_len, padding='post')
padded_sequences_valid = pad_sequences(sequences_valid, maxlen=max_len, padding='post')
padded_sequences_test = pad_sequences(sequences_test, maxlen=max_len, padding='post')

# Step 2: One-Hot Encoding
num_unique_emojis = len(tokenizer.word_index) + 1  # Unique emojis in the tokenizer

one_hot_encoded_sequences_train = np.array([to_categorical(seq, num_classes=num_unique_emojis) for seq in padded_sequences_train])
one_hot_encoded_sequences_valid = np.array([to_categorical(seq, num_classes=num_unique_emojis) for seq in padded_sequences_valid])
one_hot_encoded_sequences_test = np.array([to_categorical(seq, num_classes=num_unique_emojis) for seq in padded_sequences_test])

# Convert labels to numpy array
y_train = np.array(train_emoticon_Y)
y_valid = np.array(valid_emoticon_Y)

# Check input and label shapes before training
print(f"Input shape: {one_hot_encoded_sequences_train.shape}, Label shape: {y_train.shape}")
print(f"Validation input shape: {one_hot_encoded_sequences_valid.shape}, Validation label shape: {y_valid.shape}")

dataset1_model = load_model('dataset1_model.h5')

print(accuracy_score(y_valid, dataset1_model.predict(one_hot_encoded_sequences_valid))

test_predictions = dataset1_model.predict(one_hot_encoded_sequences_test)
test_predictions = np.round(test_predictions).flatten()  # Convert probabilities to binary predictions (0 or 1)

# Save predictions to a file
with open('pred_emoticon.txt', 'w') as f:
    for pred in test_predictions:
        f.write(f"{int(pred)}\n")

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, f1_score
import time

# Load feature dataset
train_feat = np.load("train_feature.npz", allow_pickle=True)
train_feat_X = train_feat['features']
train_feat_Y = train_feat['label']

valid_feat = np.load("valid_feature.npz", allow_pickle=True)
valid_feat_X = valid_feat['features']
valid_feat_Y = valid_feat['label']

test_feat_X = np.load("test_feature.npz", allow_pickle=True)['features']

# Flatten the 3D matrix to create a 2D matrix
train_feat_X_flat = train_feat_X.reshape(train_feat_X.shape[0], -1)  # Shape: (num_samples, 13 * 786)
valid_feat_X_flat = valid_feat_X.reshape(valid_feat_X.shape[0], -1)  # Shape: (num_samples, 13 * 786)
test_feat_X_flat = test_feat_X.reshape(test_feat_X.shape[0], -1)  # Shape: (num_samples, 13 * 786)

# Normalize the flattened features
scaler = StandardScaler()
train_feat_X = scaler.fit_transform(train_feat_X_flat)
valid_feat_X = scaler.transform(valid_feat_X_flat)
test_feat_X = scaler.transform(test_feat_X_flat)

# Initialize the SVM model
svm_model = SVC()

# Use 80% of the training data
X_sample, _, y_sample, _ = train_test_split(
    train_feat_X, train_feat_Y, train_size=0.8, stratify=train_feat_Y, random_state=42
)

# Train the model on the sampled subset
svm_model.fit(X_sample, y_sample)

# Predict on the validation set
predictions_valid = svm_model.predict(valid_feat_X)

# Calculate metrics for validation set
accuracy_valid = accuracy_score(valid_feat_Y, predictions_valid)
f1_valid = f1_score(valid_feat_Y, predictions_valid, average='weighted')
precision_valid = precision_score(valid_feat_Y, predictions_valid, average='weighted')

print(f"SVM: Validation Accuracy: {accuracy_valid:.4f}, F1 Score: {f1_valid:.4f}, Precision: {precision_valid:.4f}")

# Predict on the test set
predictions_test = svm_model.predict(test_feat_X)

# Save the predictions to a CSV file
predictions_df = pd.DataFrame(predictions_test, columns=['Predicted_Label'])
predictions_df.to_csv('pred_feat.txt', index=False)
print("Test predictions saved to 'pred_feat.csv'")

import numpy as np
import pandas as pd
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Load Training and Validation Datasets for Emoticons
train_emoticon_df = pd.read_csv("train_emoticon.csv")
valid_emoticon_df = pd.read_csv("valid_emoticon.csv")

# Extract the input emoticons and labels
train_emoticon_X = train_emoticon_df['input_emoticon'].tolist()  # List of emoticon strings
train_emoticon_Y = train_emoticon_df['label'].tolist()  # Labels

valid_emoticon_X = valid_emoticon_df['input_emoticon'].tolist()  # Validation set emoticons
valid_emoticon_Y = valid_emoticon_df['label'].tolist()  # Validation set labels

# Step 1: Split Emoticons and One-Hot Encoding
def split_emoticons(emoticon_str):
    return list(emoticon_str)  # Split the string into a list of individual emoticons

# Apply the splitting function
train_emoticon_split = [split_emoticons(emoticon_str) for emoticon_str in train_emoticon_X]
valid_emoticon_split = [split_emoticons(emoticon_str) for emoticon_str in valid_emoticon_X]

# One-hot encoding for the split emoticons (both training and validation)
encoder = OneHotEncoder(sparse=False)
train_emoticon_encoded = encoder.fit_transform(train_emoticon_split)
valid_emoticon_encoded = encoder.transform(valid_emoticon_split)

# Step 2: Preprocessing Deep Features (FLATTENING instead of averaging)
train_npz = np.load("train_feature.npz")
valid_npz = np.load("valid_feature.npz")

train_deep_features = train_npz['features']  # shape: (n_samples, 13, 786)
valid_deep_features = valid_npz['features']

# Flatten the 13x786 matrix into a single vector (13 * 786 = 10218)
train_deep_features_flattened = train_deep_features.reshape(train_deep_features.shape[0], -1)
valid_deep_features_flattened = valid_deep_features.reshape(valid_deep_features.shape[0], -1)

# Step 3: Preprocessing Text Sequence Features
train_text_df = pd.read_csv("train_text_seq.csv")
valid_text_df = pd.read_csv("valid_text_seq.csv")

train_text_X = train_text_df['input_str'].tolist()
valid_text_X = valid_text_df['input_str'].tolist()

# Function to one-hot encode the 50-digit text sequence
def one_hot_encode_digit_string(input_str, max_digit=9):
    one_hot_encoded = np.zeros((len(input_str), max_digit + 1))
    for idx, char in enumerate(input_str):
        one_hot_encoded[idx, int(char)] = 1
    return one_hot_encoded.flatten()

train_text_encoded = np.array([one_hot_encode_digit_string(seq) for seq in train_text_X])
valid_text_encoded = np.array([one_hot_encode_digit_string(seq) for seq in valid_text_X])

# Set the proportion to 80%
proportion = 0.8

# Subsample the training data for 80%
subset_size = int(proportion * len(train_emoticon_encoded))

# Concatenate features (subset of the data)
X_train = np.concatenate([
    train_emoticon_encoded[:subset_size],
    train_deep_features_flattened[:subset_size],
    train_text_encoded[:subset_size]
], axis=1)

y_train = np.array(train_emoticon_Y[:subset_size])

# Concatenate validation features
X_valid = np.concatenate([valid_emoticon_encoded, valid_deep_features_flattened, valid_text_encoded], axis=1)
y_valid = np.array(valid_emoticon_Y)

# Step 6: Standardize the Features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_valid_scaled = scaler.transform(X_valid)

# Step 7: Train the Classifier (SVM in this case)
classifier = SVC(kernel='linear', random_state=42)
classifier.fit(X_train_scaled, y_train)

# Step 8: Make Predictions on Validation Set and Evaluate
y_pred = classifier.predict(X_valid_scaled)
accuracy = accuracy_score(y_valid, y_pred)

# Load the test dataset (assuming it exists)
test_emoticon_df = pd.read_csv("test_emoticon.csv")
test_emoticon_X = test_emoticon_df['input_emoticon'].tolist()
test_emoticon_split = [split_emoticons(emoticon_str) for emoticon_str in test_emoticon_X]
test_emoticon_encoded = encoder.transform(test_emoticon_split)

# Preprocess deep features for the test set (assuming it's available)
test_npz = np.load("test_feature.npz")
test_deep_features = test_npz['features']
test_deep_features_flattened = test_deep_features.reshape(test_deep_features.shape[0], -1)

# Preprocess text sequence features for the test set
test_text_df = pd.read_csv("test_text_seq.csv")
test_text_X = test_text_df['input_str'].tolist()
test_text_encoded = np.array([one_hot_encode_digit_string(seq) for seq in test_text_X])

# Concatenate test features
X_test = np.concatenate([test_emoticon_encoded, test_deep_features_flattened, test_text_encoded], axis=1)

# Standardize the test features
X_test_scaled = scaler.transform(X_test)

# Make predictions on the test set
test_predictions = classifier.predict(X_test_scaled)

# Save the predictions to a text file
test_predictions_df = pd.DataFrame(test_predictions, columns=['Predicted_Label'])
test_predictions_df.to_csv('test_predictions.txt', sep='\t', index=False)

# The test predictions are now saved in 'test_predictions.txt'