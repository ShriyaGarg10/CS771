{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yp5LXPZzqbEU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms, models\n",
        "from torchvision.models import ConvNeXt_Tiny_Weights\n",
        "import torch.nn as nn\n",
        "\n",
        "# 1. Data Loading Functions\n",
        "def load_labeled_data(file_path):\n",
        "    \"\"\"Loads the labeled dataset (D1).\"\"\"\n",
        "    data_dict = torch.load(file_path)\n",
        "    return data_dict['data'], data_dict['targets']\n",
        "\n",
        "def load_unlabeled_data(file_path):\n",
        "    \"\"\"Loads the unlabeled datasets (D2 to D10).\"\"\"\n",
        "    data_dict = torch.load(file_path)\n",
        "    return data_dict['data']\n",
        "\n",
        "# 2. Image Preprocessing\n",
        "transform_pipeline = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet Stats\n",
        "])\n",
        "\n",
        "def preprocess_images(images):\n",
        "    \"\"\"Preprocess images using the defined transform pipeline.\"\"\"\n",
        "    return torch.stack([transform_pipeline(image / 255.0) for image in images])\n",
        "\n",
        "# 3. Feature Extraction Model\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        convnext_model = models.convnext_tiny(weights=ConvNeXt_Tiny_Weights.IMAGENET1K_V1)\n",
        "        self.feature_extractor = nn.Sequential(*list(convnext_model.children())[:-2])  # Remove final layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        return x.flatten(start_dim=1)  # Flatten the feature map\n",
        "\n",
        "embedder_model = FeatureExtractor().eval()\n",
        "\n",
        "# 4. Embedding Extraction\n",
        "def get_embeddings(dataset, embedder_model, batch_size=32):\n",
        "    \"\"\"Extract embeddings from a dataset using the pre-trained feature extractor.\"\"\"\n",
        "    embeddings = []\n",
        "    with torch.no_grad():\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        dataset = dataset.to(device)\n",
        "        embedder_model = embedder_model.to(device)\n",
        "\n",
        "        for i in tqdm(range(0, len(dataset), batch_size), desc=\"Extracting Embeddings\"):\n",
        "            batch = dataset[i:i + batch_size]\n",
        "            embeddings.append(embedder_model(batch).cpu().numpy())\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "# 5. Prototypes-based Classification Model\n",
        "class PrototypeClassifier:\n",
        "    def __init__(self):\n",
        "        self.class_prototypes = {}\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fits the model by computing class prototypes.\"\"\"\n",
        "        for label in np.unique(y):\n",
        "            self.class_prototypes[label] = np.mean(X[y == label], axis=0)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predicts the class by finding the closest prototype.\"\"\"\n",
        "        predictions = []\n",
        "        for instance in X:\n",
        "            distances = {label: np.linalg.norm(instance - prototype) for label, prototype in self.class_prototypes.items()}\n",
        "            predictions.append(min(distances, key=distances.get))\n",
        "        return np.array(predictions)\n",
        "\n",
        "# 6. Function to Update Model with Pseudo-Labels\n",
        "def update_classifier_with_pseudo_labels(current_classifier, new_data, batch_size=32):\n",
        "    \"\"\"\n",
        "    Updates the classifier with pseudo-labeled data.\n",
        "\n",
        "    Args:\n",
        "        current_classifier: The current classifier model.\n",
        "        new_data: New unlabeled data.\n",
        "        batch_size: Batch size for embedding extraction.\n",
        "\n",
        "    Returns:\n",
        "        Updated classifier and the predicted labels.\n",
        "    \"\"\"\n",
        "    # Get embeddings for the new data\n",
        "    new_data_embeddings = get_embeddings(new_data, embedder_model, batch_size=batch_size)\n",
        "\n",
        "    # Predict pseudo-labels for the new data\n",
        "    predicted_labels = current_classifier.predict(new_data_embeddings)\n",
        "\n",
        "    # Create and train a new model using pseudo-labeled data\n",
        "    updated_classifier = PrototypeClassifier()\n",
        "    updated_classifier.fit(new_data_embeddings, predicted_labels)\n",
        "\n",
        "    return updated_classifier\n",
        "\n",
        "# 7. Sequential Training with Memory Efficiency\n",
        "train_datasets_paths = [f\"/kaggle/input/cs771-mp2/dataset/part_one_dataset/train_data/{i}_train_data.tar.pth\" for i in range(1, 11)]\n",
        "eval_datasets_paths = [f\"/kaggle/input/cs771-mp2/dataset/part_one_dataset/eval_data/{i}_eval_data.tar.pth\" for i in range(1, 11)]\n",
        "\n",
        "# Initialize models list and accuracy matrix\n",
        "trained_models = []\n",
        "accuracy_table = np.zeros((20, 20))\n",
        "\n",
        "# Train the first model f1 on D1\n",
        "print(\"Training model f1...\")\n",
        "train_data, train_labels = load_labeled_data(train_datasets_paths[0])  # Load labeled data for D1\n",
        "train_data = preprocess_images(torch.tensor(train_data).permute(0, 3, 1, 2))\n",
        "train_labels = torch.tensor(train_labels)\n",
        "\n",
        "train_embeddings = get_embeddings(train_data, embedder_model)\n",
        "model_f1 = PrototypeClassifier()\n",
        "model_f1.fit(train_embeddings, train_labels.numpy())\n",
        "trained_models.append(model_f1)\n",
        "\n",
        "# Evaluate model f1\n",
        "for j, eval_file in enumerate(eval_datasets_paths[:1]):  # Only evaluate on D̂1\n",
        "    eval_data, eval_labels = load_labeled_data(eval_file)\n",
        "    eval_data = preprocess_images(torch.tensor(eval_data).permute(0, 3, 1, 2))\n",
        "    eval_embeddings = get_embeddings(eval_data, embedder_model)\n",
        "    eval_labels = torch.tensor(eval_labels)\n",
        "\n",
        "    predictions = model_f1.predict(eval_embeddings)\n",
        "    accuracy = accuracy_score(eval_labels.numpy(), predictions)\n",
        "    accuracy_table[0, j] = accuracy\n",
        "    print(f\"Accuracy of f1 on D̂{j+1}: {accuracy:.4f}\")\n",
        "\n",
        "# Sequential Training for models f2 to f10\n",
        "for i in range(1, 10):\n",
        "    print(f\"Training model f{i+1}...\")\n",
        "    train_data = load_unlabeled_data(train_datasets_paths[i])  # Load unlabeled data for D2, ..., D10\n",
        "    train_data = preprocess_images(torch.tensor(train_data).permute(0, 3, 1, 2))\n",
        "\n",
        "    # Update the model with pseudo-labels\n",
        "    current_model = trained_models[-1]\n",
        "    updated_model = update_classifier_with_pseudo_labels(current_model, train_data)\n",
        "    trained_models.append(updated_model)\n",
        "\n",
        "    # Evaluate the updated model on previous datasets\n",
        "    for j, eval_file in enumerate(eval_datasets_paths[:i + 1]):\n",
        "        eval_data, eval_labels = load_labeled_data(eval_file)\n",
        "        eval_data = preprocess_images(torch.tensor(eval_data).permute(0, 3, 1, 2))\n",
        "        eval_embeddings = get_embeddings(eval_data, embedder_model)\n",
        "        eval_labels = torch.tensor(eval_labels)\n",
        "\n",
        "        predictions = updated_model.predict(eval_embeddings)\n",
        "        accuracy = accuracy_score(eval_labels.numpy(), predictions)\n",
        "        accuracy_table[i, j] = accuracy\n",
        "        print(f\"Accuracy of f{i+1} on D̂{j+1}: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "# Task 2 - Updated Code for f11 to f20\n",
        "train_datasets_part_two = [f\"{i}_train_data.tar.pth\" for i in range(1, 11)]\n",
        "eval_datasets_part_two = [f\"{i}_eval_data.tar.pth\" for i in range(1, 11)]\n",
        "\n",
        "# Load data for part two\n",
        "def load_part_two_data(file_path):\n",
        "    \"\"\"Loads datasets from part two (D11 to D20).\"\"\"\n",
        "    data_dict = torch.load(file_path)\n",
        "    return data_dict['data']  # Only the data, no labels\n",
        "\n",
        "# Combine part one and part two datasets\n",
        "combined_train_datasets = train_datasets_paths + train_datasets_part_two\n",
        "combined_eval_datasets = eval_datasets_paths + eval_datasets_part_two\n",
        "\n",
        "# Initialize accuracy table for part two\n",
        "accuracy_table_part_two = np.zeros((10, 20))\n",
        "\n",
        "# Train models f11 to f20\n",
        "for i in range(10, 20):\n",
        "    print(f\"Training model f{i+1}...\")\n",
        "\n",
        "    # Load unlabeled data D11 to D20\n",
        "    train_data = load_unlabeled_data(combined_train_datasets[i])\n",
        "    train_data = preprocess_images(torch.tensor(train_data).permute(0, 3, 1, 2))\n",
        "\n",
        "    # Get pseudo-labels for the new data\n",
        "    current_model = trained_models[-1]\n",
        "    pseudo_labels = current_model.predict(get_embeddings(train_data, embedder_model))\n",
        "\n",
        "    # Train the new model using pseudo-labeled data\n",
        "    updated_model = PrototypeClassifier()\n",
        "    updated_model.fit(get_embeddings(train_data, embedder_model), pseudo_labels)\n",
        "    trained_models.append(updated_model)\n",
        "\n",
        "    # Evaluate the model on all held-out datasets\n",
        "    for j, eval_file in enumerate(combined_eval_datasets[:i + 1]):\n",
        "        eval_data, eval_labels = load_labeled_data(eval_file)\n",
        "        eval_data = preprocess_images(torch.tensor(eval_data).permute(0, 3, 1, 2))\n",
        "        eval_embeddings = get_embeddings(eval_data, embedder_model)\n",
        "        eval_labels = torch.tensor(eval_labels)\n",
        "\n",
        "        predictions = updated_model.predict(eval_embeddings)\n",
        "        accuracy = accuracy_score(eval_labels.numpy(), predictions)\n",
        "        accuracy_table_part_two[i-10, j] = accuracy\n",
        "        print(f\"Accuracy of f{i+1} on D̂{j+1}: {accuracy:.4f}\")\n",
        "\n",
        "# Print final accuracy table for models f11 to f20\n",
        "print(\"Final Accuracy Table (Models vs Held-out Datasets for Part Two):\")\n",
        "print(accuracy_table_part_two)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TASK 2\n",
        "Transfer learning: The use of the pre-trained ConvNeXt model for feature extraction can be considered as a form of transfer learning. The model adapts its learned knowledge from the initial dataset (ImageNet) to the specific task at hand. This adaptation improves the model’s ability to generalize to new, unlabeled data (as represented in datasets D2 to D10)."
      ],
      "metadata": {
        "id": "ih4Xg-Z_sEVN"
      }
    }
  ]
}